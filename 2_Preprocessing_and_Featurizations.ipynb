{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center> <h1> Freesound Audio Tagging </h1> </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Preprocessing and Featurizations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following featurizations are possible:\n",
    "1. Removing leading and trailing silences (noises below 60 dB)<br><br>\n",
    "2. Resampling the audio clip\n",
    " - Current sampling rate = 44.1 kHz\n",
    " - Acc. to some kaggle kernels, we can change the sampling rate to 16 kHz without losing much information resulting in faster computations<br><br>\n",
    "3. Random offsetting/padding:\n",
    " - After removing leading and trailing noises, the lengths of audio clips vary from 0-30 seconds, one idea is to take all clips of same lengths, i.e. 15 seconds, so, clips longer than that, we choose a random sample of length 15 seconds and for clips shorter than that, we pad the clip with zeros on either side.\n",
    " - By choosing the random offset, we perform a kind of randomization which helps in controlling overfitting and when all the clips become of same length, then, it becomes easy to feed data to the model\n",
    " - By the above step, all clips become of same length (15 seconds)<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Libraries and Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "from pathlib import Path\n",
    "import os\n",
    "import shutil\n",
    "import librosa\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combined = pd.read_csv(r'../train_curated.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating a list of all class labels in sorted order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "LABELS=set()\n",
    "all_labels = list(df_combined['labels'])\n",
    "for row in all_labels:\n",
    "    for lab in row.split(r','):\n",
    "        LABELS.add(lab)\n",
    "\n",
    "LABELS=list(LABELS)\n",
    "LABELS.sort()\n",
    "joblib.dump(LABELS, 'labels.joblib');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Cross Validation Split\n",
    "We perform a 70-30 train cross validate split which is not random, but, stratified such that both train and validation dataset contain approximately the same distribution of number of labels per clip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_labels(label_string):\n",
    "    '''\n",
    "    Descrption -> Returns the total count of labels in the given comma separated label_string\n",
    "    Input -> String containing all the labels corr. the clip in CSV format (\"label1,label2,label3\")\n",
    "    Output -> Number of unique labels in the label_string (eg. 3)\n",
    "    '''\n",
    "    return len(label_string.split(','))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combined['label_count'] = df_combined['labels'].apply(count_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    4269\n",
       "2     627\n",
       "3      69\n",
       "4       4\n",
       "6       1\n",
       "Name: label_count, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_combined['label_count'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "ALL_FILES_DIRECTORY = r'all_data/'\n",
    "TRAIN_FILES_DIRECTORY = r'train_data/'\n",
    "VAL_FILES_DIRECTORY = r'val_data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There is only one point with 6 labels which can be considered as an outlier hence we delete the corresponding clip\n",
    "fname_to_be_deleted = (df_combined[df_combined['label_count'] == 6]['fname']).values[0]\n",
    "fname_path = ALL_FILES_DIRECTORY + fname_to_be_deleted\n",
    "os.remove(fname_path)\n",
    "df_combined = df_combined[df_combined['label_count'] < 6]\n",
    "y_count=np.array(df_combined['label_count'])\n",
    "df_train, df_val = train_test_split(df_combined, stratify=y_count, test_size=0.3, random_state=21)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_percentage_of_labels_in_a_dataframe(df):\n",
    "    '''\n",
    "    Get the number of labels for each clip and the percentage of clips having it\n",
    "    Input  -> Pandas dataframe containing \"fname\" (filename), \"label\" (ground truth labels) and \n",
    "             \"label_count\" (number of ground truth labels for the fname)\n",
    "    Output -> Pandas dataframe containing \"Number of labels\" (each unique value in the \"label_count\") and\n",
    "              \"Percentage of clips\" having it\n",
    "    '''\n",
    "    col1=list(df['label_count'].unique())\n",
    "    col2=list(round(df['label_count'].value_counts()/sum(df['label_count'].value_counts())*100, 2))\n",
    "    df_labels_percentage = pd.DataFrame({'Number of labels' : col1, 'Percentage of clips having it' : col2})\n",
    "    return df_labels_percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Number of labels</th>\n",
       "      <th>Percentage of clips having it</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>85.91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>12.62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1.38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0.09</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Number of labels  Percentage of clips having it\n",
       "0                 1                          85.91\n",
       "1                 2                          12.62\n",
       "2                 3                           1.38\n",
       "3                 4                           0.09"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_labels_percentage_train = get_distribution_of_labels_in_a_dataframe(df_train)\n",
    "df_labels_percentage_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Number of labels</th>\n",
       "      <th>Percentage of clips having it</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>85.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>12.61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1.41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0.07</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Number of labels  Percentage of clips having it\n",
       "0                 1                          85.92\n",
       "1                 2                          12.61\n",
       "2                 3                           1.41\n",
       "3                 4                           0.07"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_labels_percentage_val = get_distribution_of_labels_in_a_dataframe(df_val)\n",
    "df_labels_percentage_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Moving the .wav files present in df_val to the VAL_FILES_DIRECTORY\n",
    "df_val_filenames = set(df_val['fname'])\n",
    "all_files = os.listdir(ALL_FILES_DIRECTORY)\n",
    "\n",
    "for file in all_files:\n",
    "    if file in df_val_filenames:\n",
    "        src_path = os.path.join(ALL_FILES_DIRECTORY, file)\n",
    "        dst_path = os.path.join(VAL_FILES_DIRECTORY, file)\n",
    "        shutil.move(src_path, dst_path)\n",
    "        \n",
    "os.rename(ALL_FILES_DIRECTORY, TRAIN_FILES_DIRECTORY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving for future use\n",
    "df_train.to_csv(r'df_train.csv', index=False)\n",
    "df_val.to_csv(r'df_val.csv', index=False)\n",
    "df_combined.to_csv('df_combined.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Configuration class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Configuration object stores those learning parameters that are shared between data generators, models, and training functions. Basically, these are the global variables as far as training is considered. <br><br>\n",
    "Various parameters used here are: <br>\n",
    "1. **Sampling rate:** Number of samples picked up per second. The original sample rate of the data was 44.1 kHz, i.e., 44,100 samples were picked up per second while recording the audio. We choose a lower sampling rate of 16 kHz, i.e., 16,000 samples are picked up per second. This is a kind of downsampling which helps us to speed up model training due to computational constraints.<br><br>\n",
    "2. **Audio Duration**: For feeding data to the model, we need each datapoint to be of the same dimension, this could be possible if each clip were to be of the same audio length (i.e. same total number of samples). In the EDA, we saw that, this is not the case. So, we choose an audio duration of 15 seconds for each clip, which means regardless of the actual length of the audio clip, we'll select 16,000 * 15 = 2,40,000 samples for each datapoint. <br>\n",
    " - For the clips with actual length shorter than 15 seconds, we'll pad the clips on either side with silences (zeroes) so that the duration becomes exactly 15 seconds.\n",
    " - For the clips having an audio duration > 15 seconds, we choose a random sample of 15 seconds from the clip. This also acts like data augmentation at training time which helps in controlling overfitting <br><br>\n",
    "3. **n_classes:** This is the number of unique labels in the modified dataset (containing only datapoints with single labels) <br><br>\n",
    "4. **Audio length:** This is the total number of samples present in the clip. If the sampling rate is S (S samples are picked up per second) and the duration of the clip is t seconds, the total number of samples present in the clip = S * t <br><br>\n",
    "5. **Dimensionality:** The above audio length decides the dimensionality of each datapoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Config class is used to share the global parameters across various functions\n",
    "class Config():\n",
    "    def __init__(self,\n",
    "                 sampling_rate=None,\n",
    "                 audio_duration=None):\n",
    "        \n",
    "        self.sampling_rate = sampling_rate\n",
    "        self.audio_duration = audio_duration\n",
    "        self.n_classes = len(df_combined['labels'].value_counts())\n",
    "        self.audio_length = self.sampling_rate * self.audio_duration\n",
    "        self.dim = (self.audio_length, 1)\n",
    "        \n",
    "config = Config(sampling_rate=16000, audio_duration=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the preprocessing function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_initial(config, data_dir, dest_folder):\n",
    "    '''\n",
    "    Objective ->\n",
    "    This function performs the preprocessing on each audio clip in the data_dir\n",
    "    \n",
    "    Input ->\n",
    "    config: An instance of the above Config() class which is used to determine the configuration parameters used for\n",
    "    preprocessing\n",
    "    \n",
    "    data_dir: Path of the training/testing data folder which contains the .wav files\n",
    "    \n",
    "    Processing ->\n",
    "    1. Load each of the .wav file present in data_dir folder in a NumPy array\n",
    "    2. Trim the leading and trailing silences (sounds below 60 dB loudness)\n",
    "    3. Select a fixed length random sample of 15 seconds from each clip to ensure uniform dimensions while training the model\n",
    "       For clips shroter than 15 seconds, pad the NumPy array with zeros uniformly on both ends\n",
    "    4. Store the modified clip in \"dest_folder\"\n",
    "    '''\n",
    "    input_length = config.audio_length\n",
    "    \n",
    "    # Iterate through each file in the \"data_dir\" which contains all the .wav files\n",
    "    for file in os.listdir(data_dir):\n",
    "        filepath = data_dir + \"//\" + file\n",
    "        # Load the .wav file into a numpy array \"data\" using 16 khz sampling rate and \"kaiser_fast\" resolution which quickly\n",
    "        # loads the file\n",
    "        data, _ = librosa.core.load(filepath,\n",
    "                                    sr=config.sampling_rate,\n",
    "                                    res_type='kaiser_fast')\n",
    "        \n",
    "        # Trim the leading and trailing silences, i.e., sounds below 60 dB of loudness (inaudible to human ear)\n",
    "        data, _ = librosa.effects.trim(data, top_db=60)\n",
    "        \n",
    "        # Random offset / Padding\n",
    "        # Case 1: Audio longer than \"input_length\" seconds -> We choose a random subsample of data of \"input_length\" seconds\n",
    "        if len(data) > input_length:\n",
    "            pad_flag=0\n",
    "            max_offset = len(data) - input_length\n",
    "            offset = np.random.randint(max_offset)\n",
    "            data = data[offset:(input_length+offset)]\n",
    "            \n",
    "        # Case 2: Audio shorter than \"input_length\" seconds -> Padding with zeroes required on either side of the clip\n",
    "        elif input_length > len(data):\n",
    "            pad_flag=1\n",
    "            max_offset = input_length - len(data)\n",
    "            offset = np.random.randint(max_offset)\n",
    "                \n",
    "        # Case 3: Audio is exactly \"input_length\" seconds long -> No change is required\n",
    "        else:\n",
    "            pad_flag = 0\n",
    "            offset = 0\n",
    "            \n",
    "        if pad_flag:\n",
    "            data = np.pad(data, (offset, input_length - len(data) - offset), \"constant\")\n",
    "     \n",
    "        dest_path = dest_folder + file\n",
    "        \n",
    "        write(dest_path, config.sampling_rate, data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing for the train and validation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_FILES_DIRECTORY = r'train_data/'\n",
    "TRAIN_DEST_DIRECTORY = r'preprocessed_files_train/'\n",
    "\n",
    "VAL_FILES_DIRECTORY = r'val_data/'\n",
    "VAL_DEST_DIRECTORY = r'preprocessed_files_val/'\n",
    "\n",
    "preprocess_initial(config, TRAIN_FILES_DIRECTORY, TRAIN_DEST_DIRECTORY)\n",
    "preprocess_initial(config, VAL_FILES_DIRECTORY, VAL_DEST_DIRECTORY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing for the test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_FILES_DIRECTORY = r'test_data/'\n",
    "TEST_DEST_DIRECTORY = r'preprocessed_files_test/'\n",
    "\n",
    "preprocess_initial(config, TEST_FILES_DIRECTORY, TEST_DEST_DIRECTORY)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
